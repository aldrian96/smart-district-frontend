{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13939647,"sourceType":"datasetVersion","datasetId":8883786},{"sourceId":4534,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3326,"modelId":986},{"sourceId":4537,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":3329,"modelId":986},{"sourceId":662613,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":501331,"modelId":516512}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CSIRO Competition Solution Notebook","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:51.263363Z","iopub.execute_input":"2025-12-01T13:18:51.263543Z","iopub.status.idle":"2025-12-01T13:18:51.758550Z","shell.execute_reply.started":"2025-12-01T13:18:51.263527Z","shell.execute_reply":"2025-12-01T13:18:51.757693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/sam-optim\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:51.763080Z","iopub.execute_input":"2025-12-01T13:18:51.763372Z","iopub.status.idle":"2025-12-01T13:18:51.768965Z","shell.execute_reply.started":"2025-12-01T13:18:51.763346Z","shell.execute_reply":"2025-12-01T13:18:51.768176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ntorch.cuda.is_available()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:51.769786Z","iopub.execute_input":"2025-12-01T13:18:51.770367Z","iopub.status.idle":"2025-12-01T13:18:53.501177Z","shell.execute_reply.started":"2025-12-01T13:18:51.770339Z","shell.execute_reply":"2025-12-01T13:18:53.500522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sam import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:53.501948Z","iopub.execute_input":"2025-12-01T13:18:53.502312Z","iopub.status.idle":"2025-12-01T13:18:53.508063Z","shell.execute_reply.started":"2025-12-01T13:18:53.502293Z","shell.execute_reply":"2025-12-01T13:18:53.507457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Visualize target distributions\n# fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n# axes = axes.ravel()\n\n# target_names = df['target_name'].unique()\n# for idx, target in enumerate(target_names[:6]):\n#     data = df[df['target_name'] == target]['target']\n#     axes[idx].hist(data, bins=50, edgecolor='black', alpha=0.7)\n#     axes[idx].set_title(f'{target}\\nMean: {data.mean():.2f}, Std: {data.std():.2f}')\n#     axes[idx].set_xlabel('Target Value (g)')\n#     axes[idx].set_ylabel('Frequency')\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-12-01T13:18:53.508884Z","iopub.execute_input":"2025-12-01T13:18:53.509213Z","iopub.status.idle":"2025-12-01T13:18:53.519328Z","shell.execute_reply.started":"2025-12-01T13:18:53.509190Z","shell.execute_reply":"2025-12-01T13:18:53.518635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Transform\n\nfrom torchvision.transforms import v2\nimport torch\n\n# to_tensor = v2.ToTensor()\n# img_tensor = to_tensor(img)\n\ndtype = torch.float32\nimg_size = (224, 224)\nimage_transform = v2.Compose([\n    v2.ToImage(),\n    v2.ToDtype(dtype, scale=True),    \n    v2.Resize(img_size),\n    v2.RandomHorizontalFlip(p=0.5),\n    v2.RandomVerticalFlip(p=0.5),\n    v2.RandomRotation(5, interpolation=v2.InterpolationMode.BILINEAR),\n    v2.ColorJitter(\n        brightness=0.25,\n        contrast=0.25,\n        saturation=0.25,\n        hue=0.05,\n    ),\n    # v2.RandomAdjustSharps\n    v2.Normalize(mean=[0.485, 0.456, 0.406],\n                 std=[0.229, 0.224, 0.225]),\n])\n\nval_transform = v2.Compose([\n    v2.ToImage(),\n    v2.ToDtype(dtype, scale=True),\n    v2.Resize(img_size),\n    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n\ndef numeric_transform(X, X_max, X_min) -> torch.Tensor:\n    X_normalized = (X - X_min) / (X_max - X_min)\n    return X_normalized\n\ndef target_transform(targets) -> torch.Tensor:\n    return torch.log1p(targets)\n\ndef target_untransform(targets) -> torch.Tensor:\n    return torch.expm1(targets)\n\ndef categorical_transform(row) -> torch.Tensor:\n    return row","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:53.520150Z","iopub.execute_input":"2025-12-01T13:18:53.520655Z","iopub.status.idle":"2025-12-01T13:18:55.064291Z","shell.execute_reply.started":"2025-12-01T13:18:53.520633Z","shell.execute_reply":"2025-12-01T13:18:55.063487Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Set","metadata":{}},{"cell_type":"code","source":"\nfrom torch.utils.data import Dataset\nfrom torchvision.io import decode_image\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport pandas as pd\n\nclass Image2BioMassTrainValDataset(Dataset):\n    \n    def __init__(self, dataset_path, img_transform=None, numeric_transform=None,categorical_transform=None, target_transform=None):\n        \n        self.df = self.process_df(dataset_path)\n        self.dataset_path = dataset_path\n        self.img_transform = img_transform\n        self.target_transform = target_transform\n        self.numeric_transform = numeric_transform\n        self.categorical_transform = categorical_transform\n        self.targets = self.df.loc[:, [\"Dry_Green_g\", \"Dry_Dead_g\", \"Dry_Clover_g\"]]\n\n\n    def process_df(self, dataset_path):\n        self.le_date = LabelEncoder()\n        self.le_state = LabelEncoder()\n        self.le_species = LabelEncoder()\n\n        df = pd.read_csv(os.path.join(dataset_path, \"train.csv\"))\n        df['base_sample_id'] = df['sample_id'].str.split('__').str[0]\n        df = df.pivot_table(\n        index=['base_sample_id', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'],\n        columns='target_name',\n        values='target'\n        ).reset_index()\n        df[\"Sampling_Date\"] = self.le_date.fit_transform(df[\"Sampling_Date\"])\n        df[\"State\"] = self.le_state.fit_transform(df[\"State\"])\n        df[\"Species\"] = self.le_species.fit_transform(df[\"Species\"])\n        # display(df)\n        return df\n\n    def __len__(self):\n        return len(self.df)\n\n    def get_cat_features(self):\n        return [\"Sampling_Date\", \"State\", \"Species\"]\n    \n    def get_cat_vocab_sizes(self):\n        results = []\n\n        for i in self.get_cat_features():\n            results.append(len(self.df[i].unique()))\n        return results\n\n    def __getitem__(self, idx):\n        # B = batch_size\n        # display(self.df)\n        img_path = os.path.join(self.dataset_path, self.df.loc[idx, 'image_path'])\n        image = decode_image(img_path)\n        # display(self.df)\n        numeric_features = torch.tensor([\n            self.df.loc[idx, \"Pre_GSHH_NDVI\"],\n            self.df.loc[idx, \"Height_Ave_cm\"],\n        ], dtype=torch.float32)\n\n        categorical_features = torch.tensor([\n            self.df.loc[idx, \"Sampling_Date\"],\n            self.df.loc[idx, \"State\"],\n            self.df.loc[idx, \"Species\"],\n        ], dtype=torch.long)\n        \n\n        if self.img_transform:\n            image = self.img_transform(image)\n            \n        if self.numeric_transform:\n            # numeric_features[0] = self.numeric_transform(\n            #     numeric_features[0],\n            #     self.df.loc[:, \"Pre_GSHH_NDVI\"].max(), \n            #     self.df.loc[:, \"Pre_GSHH_NDVI\"].min()\n            # )\n            numeric_features[1] = self.numeric_transform(\n                numeric_features[1], \n                self.df.loc[:, \"Height_Ave_cm\"].max(), \n                self.df.loc[:, \"Height_Ave_cm\"].min()\n            )\n            # print(numeric_features)\n        combined_features = torch.cat([categorical_features.float(), numeric_features], dim=0)\n        # print(combined_features)\n        targets = torch.Tensor(self.targets.iloc[idx].values)\n        if self.target_transform:\n            targets = self.target_transform(targets)\n        return image, combined_features, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:55.065250Z","iopub.execute_input":"2025-12-01T13:18:55.065745Z","iopub.status.idle":"2025-12-01T13:18:55.506630Z","shell.execute_reply.started":"2025-12-01T13:18:55.065721Z","shell.execute_reply":"2025-12-01T13:18:55.506014Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Set","metadata":{}},{"cell_type":"code","source":"\n# from torch.utils.data import Dataset\n# from torchvision.io import decode_image\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import train_test_split\n# from torch.utils.data import DataLoader\n# import pandas as pd\n\n# class Image2BioMassTestFromTrainDataset(Dataset):\n    \n#     def __init__(self, dataset_path, img_transform=None, numeric_transform=None,categorical_transform=None):\n        \n#         self.df = self.process_df(dataset_path)\n#         self.dataset_path = dataset_path\n#         self.img_transform = img_transform\n#         self.numeric_transform = numeric_transform\n#         self.categorical_transform = categorical_transform\n\n#     def process_df(self, dataset_path):\n#         self.le_date = LabelEncoder()\n#         self.le_state = LabelEncoder()\n#         self.le_species = LabelEncoder()\n\n#         df = pd.read_csv(os.path.join(dataset_path, \"train.csv\"))\n#         df['base_sample_id'] = df['sample_id'].str.split('__').str[0]\n#         df = (\n#             df.assign(_val=\"\")\n#               .pivot(index=['base_sample_id', \"image_path\"],\n#                      columns='target_name',\n#                      values='_val')\n#               .reset_index()\n#         )\n\n#         return df\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def get_cat_features(self):\n#         return [\"Sampling_Date\", \"State\", \"Species\"]\n    \n#     def get_cat_vocab_sizes(self):\n#         results = []\n\n#         for i in self.get_cat_features():\n#             results.append(len(self.df[i].unique()))\n#         return results\n\n#     def __getitem__(self, idx):\n\n#         img_path = os.path.join(self.dataset_path, self.df.loc[idx, 'image_path'])\n#         image = decode_image(img_path)\n\n#         # Use val_transform for test data (no augmentation)\n#         if self.img_transform:\n#             image = self.img_transform(image)\n#         else:\n#             # Fallback basic transform if no transform provided\n#             transform = v2.Compose([\n#                 v2.ToImage(),\n#                 v2.ToDtype(dtype, scale=True),\n#                 v2.Resize((518, 518)),\n#                 v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n#             ])\n#             image = transform(image)\n\n#         combined_features = torch.zeros(5, dtype=torch.float32)\n#         sample_id = self.df.loc[idx, 'base_sample_id']\n#         return image, combined_features, sample_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:55.507335Z","iopub.execute_input":"2025-12-01T13:18:55.507699Z","iopub.status.idle":"2025-12-01T13:18:55.512238Z","shell.execute_reply.started":"2025-12-01T13:18:55.507679Z","shell.execute_reply":"2025-12-01T13:18:55.511581Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom torch.utils.data import Dataset\nfrom torchvision.io import decode_image\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport pandas as pd\n\nclass Image2BioMassTestDataset(Dataset):\n    \n    def __init__(self, dataset_path, img_transform=None, numeric_transform=None,categorical_transform=None):\n        \n        self.df = self.process_df(dataset_path)\n        self.dataset_path = dataset_path\n        self.img_transform = img_transform\n        self.numeric_transform = numeric_transform\n        self.categorical_transform = categorical_transform\n\n    def process_df(self, dataset_path):\n        self.le_date = LabelEncoder()\n        self.le_state = LabelEncoder()\n        self.le_species = LabelEncoder()\n\n        df = pd.read_csv(os.path.join(dataset_path, \"test.csv\"))\n        df['base_sample_id'] = df['sample_id'].str.split('__').str[0]\n        df = (\n            df.assign(_val=\"\")\n              .pivot(index=['base_sample_id', \"image_path\"],\n                     columns='target_name',\n                     values='_val')\n              .reset_index()\n        )\n\n        return df\n\n    def __len__(self):\n        return len(self.df)\n\n    def get_cat_features(self):\n        return [\"Sampling_Date\", \"State\", \"Species\"]\n    \n    def get_cat_vocab_sizes(self):\n        results = []\n\n        for i in self.get_cat_features():\n            results.append(len(self.df[i].unique()))\n        return results\n\n    def __getitem__(self, idx):\n\n        img_path = os.path.join(self.dataset_path, self.df.loc[idx, 'image_path'])\n        image = decode_image(img_path)\n\n        # Use val_transform for test data (no augmentation)\n        if self.img_transform:\n            image = self.img_transform(image)\n        else:\n            # Fallback basic transform if no transform provided\n            transform = v2.Compose([\n                v2.ToImage(),\n                v2.ToDtype(dtype, scale=True),\n                v2.Resize((518, 518)),\n                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n            image = transform(image)\n\n        combined_features = torch.zeros(5, dtype=torch.float32)\n        sample_id = self.df.loc[idx, 'base_sample_id']\n        return image, combined_features, sample_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:55.513134Z","iopub.execute_input":"2025-12-01T13:18:55.513433Z","iopub.status.idle":"2025-12-01T13:18:55.531682Z","shell.execute_reply.started":"2025-12-01T13:18:55.513404Z","shell.execute_reply":"2025-12-01T13:18:55.530866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = Image2BioMassTestDataset(\n    dataset_path=\"/kaggle/input/csiro-biomass/\",\n    img_transform=val_transform,  # Use val_transform (no augmentation, proper size)\n    \n)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\nnext(iter(test_dataloader))[2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:55.532452Z","iopub.execute_input":"2025-12-01T13:18:55.532732Z","iopub.status.idle":"2025-12-01T13:18:55.628466Z","shell.execute_reply.started":"2025-12-01T13:18:55.532710Z","shell.execute_reply":"2025-12-01T13:18:55.627777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Split","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Subset\n\n# Create base dataset to get indices\nbase_dataset = Image2BioMassTrainValDataset(\n    dataset_path=\"/kaggle/input/csiro-biomass/\",\n    img_transform=None,  # no transform yet\n    numeric_transform=numeric_transform,\n    target_transform=target_transform\n)\n\n# Split indices\nseed = 42\ntrain_indices, val_indices = train_test_split(\n    range(len(base_dataset)), \n    train_size=0.8, \n    shuffle=True, \n    random_state=seed\n)\n\n# Create training dataset WITH augmentation\ntrain_dataset = Image2BioMassTrainValDataset(\n    dataset_path=\"/kaggle/input/csiro-biomass/\",\n    img_transform=image_transform,  # WITH augmentation\n    numeric_transform=numeric_transform,\n    target_transform=target_transform\n)\ntrain_dataset = Subset(train_dataset, train_indices)\n\nval_dataset = Image2BioMassTrainValDataset(\n    dataset_path=\"/kaggle/input/csiro-biomass/\",\n    img_transform=val_transform,  # WITHOUT augmentation\n    numeric_transform=numeric_transform,\n    target_transform=target_transform\n)\nval_dataset = Subset(val_dataset, val_indices)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:55.631189Z","iopub.execute_input":"2025-12-01T13:18:55.631447Z","iopub.status.idle":"2025-12-01T13:18:55.690403Z","shell.execute_reply.started":"2025-12-01T13:18:55.631430Z","shell.execute_reply":"2025-12-01T13:18:55.689860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"img features:\", next(iter(train_dataloader))[0].shape)\nprint(\"tabular features:\", next(iter(train_dataloader))[1][0])\nprint(\"Dry Clover, Dry Dead, Dry Green:\", next(iter(train_dataloader))[2][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:55.691068Z","iopub.execute_input":"2025-12-01T13:18:55.691313Z","iopub.status.idle":"2025-12-01T13:18:57.129545Z","shell.execute_reply.started":"2025-12-01T13:18:55.691293Z","shell.execute_reply":"2025-12-01T13:18:57.128797Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nBATCH_SIZE=32\nHEIGHT=224\nWIDTH=224\nNUM_CHANNELS=3\nclass BackBone(nn.Module):\n\n    def __init__(self, num_channels=3):\n        super(BackBone, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=32, kernel_size=3, padding=1)\n        self.batch_norm1 = nn.BatchNorm2d(32)\n        self.activ1 = nn.GELU()\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.batch_norm2 = nn.BatchNorm2d(16)\n        self.activ2 = nn.GELU()\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=3, kernel_size=3, padding=1)\n        self.batch_norm3 = nn.BatchNorm2d(3)\n        self.activ3 = nn.GELU()\n\n        self.downsample = None\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.batch_norm1(out)\n        out = self.activ1(out)\n        out = self.conv2(out)\n        out = self.batch_norm2(out)\n        out = self.activ2(out)\n        out = self.conv3(out)\n        out = self.batch_norm3(out)\n        # print(out.shape)\n        # print(identity.shape)\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n\n        return out\n\nclass Image2BiomassModel(nn.Module):\n\n    def __init__(self):\n        super(Image2BiomassModel, self).__init__()\n        # self.backbone = Dinov2Model.from_pretrained(\n        #     \"/kaggle/input/dinov2/pytorch/base/1/\"\n        # )\n\n        self.backbone = BackBone(num_channels=3)\n        self.prelu = nn.PReLU()\n\n        # ---- MLP Head ----\n        self.noise = nn.Sequential(\n            nn.AlphaDropout(0.1),\n        )\n\n        self.fc1 = nn.Sequential(\n            nn.Linear(HEIGHT * WIDTH * NUM_CHANNELS, 512),\n            nn.BatchNorm1d(512),\n            nn.PReLU(),\n            nn.Dropout(0.4),\n        )\n\n        self.fc2 = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.PReLU(),\n            nn.Linear(256, 128),\n            nn.LayerNorm(128),\n            nn.PReLU(),\n            nn.Dropout(0.4),\n        )\n\n        self.residual = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.LayerNorm(128),\n            nn.PReLU(),\n            nn.Linear(128, 128),\n            nn.LayerNorm(128),\n        )\n        self.out = nn.Linear(128, 3)\n\n        self.criterion = nn.SmoothL1Loss(beta=0.5)\n        \n    def forward(self, x, y=None):\n        # DINOv2-giant expects normalized images and outputs [B, 1536]\n        outputs = self.backbone(x, )\n        # x = outputs.last_hidden_state[:, 0]\n\n        x = outputs.view(outputs.shape[0], -1)\n        # print()\n        x = self.noise(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n\n        res = self.residual(x)\n        x = x + res\n        x = self.prelu(x)\n        # x = F.Mish(x)\n\n        preds = self.out(x)\n\n        loss = None\n        if y is not None:\n            loss = self.criterion(preds, y)\n\n        return preds, loss\n\nsample = next(iter(train_dataloader))\nmodel = Image2BiomassModel()\n\nmodel(sample[0], sample[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:57.130459Z","iopub.execute_input":"2025-12-01T13:18:57.130800Z","iopub.status.idle":"2025-12-01T13:18:57.136367Z","shell.execute_reply.started":"2025-12-01T13:18:57.130780Z","shell.execute_reply":"2025-12-01T13:18:57.135529Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Loop","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Image2BiomassModel().to(device)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# base_optimizer = torch.optim.AdamW\n# optimizer = SAM(model.parameters(), base_optimizer, lr=1e-4, weight_decay=1e-2)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\nweights = torch.tensor([0.1, 0.1, 0.1, 0.2, 0.5], device=device)\n\ntrain_losses, val_losses = [], []\ntrain_r2_history, val_r2_history = [], []\n\n\n# eval metrics\ndef weighted_r2(y_true, y_pred, weights):\n    mean = y_true.mean(dim=0)\n    SSE = ((y_true - y_pred)**2).sum(dim=0)\n    TSS = ((y_true - mean)**2).sum(dim=0)\n    TSS = torch.clamp(TSS, min=1e-8)\n    R2 = 1 - SSE / TSS\n    R2 = torch.clamp(R2, min=-10, max=1)\n    return (R2 * weights).sum() / weights.sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:58.300628Z","iopub.execute_input":"2025-12-01T13:18:58.301039Z","iopub.status.idle":"2025-12-01T13:18:58.539044Z","shell.execute_reply.started":"2025-12-01T13:18:58.301009Z","shell.execute_reply":"2025-12-01T13:18:58.538442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\nfrom torch.nn.utils import clip_grad_norm_\n\nepochs = 100\nfor epoch in range(1, epochs+1):\n\n    model.train()\n    train_loss = 0\n    train_r2_scores = []\n\n    for imgs, _, y in tqdm(train_dataloader, desc=f\"[Train] Epoch {epoch}\"):\n\n        imgs, y = imgs.to(device), y.to(device)\n\n        # preds, loss = model(imgs, y)\n        # optimizer.zero_grad()\n        # print(loss.requires_grad)\n        # def closure():\n        #     # optimizer.zero_grad()\n        #     loss.backward()\n        #     return loss\n        # # loss.backward()\n        # optimizer.step(closure)\n        \n        # --- FIRST forward-backward (compute gradients on current weights) ---\n        preds, loss = model(imgs, y)\n\n        # ----- L1 regularization -----\n        l1_lambda = 1e-6   # YOU set this value; this is a reasonable starting point\n        reg_loss = sum(param.abs().sum() for param in model.parameters())\n        loss = loss + l1_lambda * reg_loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        # perform SAM first step (perturb weights)\n        \n        clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        # optimizer.first_step(zero_grad=True)\n\n        # # --- SECOND forward-backward on perturbed weights ---\n        # # recompute preds & loss with perturbed weights, then backprop and second step\n        # preds_2, loss_2 = model(imgs, y)\n\n        # loss_2.backward()\n        # optimizer.second_step(zero_grad=True)\n\n        train_loss += loss.item()\n\n        # ----- Add 2 more outputs (not for training) -----\n        # Order: [Dry_Green_g, Dry_Dead_g, Dry_Clover_g, GDM_g, Dry_Total_g]\n        preds5 = torch.cat([\n            preds,\n            (preds[:, 2] + preds[:, 0]).unsqueeze(1),  # GDM_g = Dry_Clover + Dry_Green\n            preds.sum(dim=1).unsqueeze(1)             # Dry_Total_g = all three\n        ], dim=1)\n\n        y5 = torch.cat([\n            y,\n            (y[:, 2] + y[:, 0]).unsqueeze(1),  # GDM_g = Dry_Clover + Dry_Green\n            y.sum(dim=1).unsqueeze(1)          # Dry_Total_g = all three\n        ], dim=1)\n\n        train_r2_scores.append(weighted_r2(y5, preds5, weights).item())\n\n    avg_train_loss = train_loss / len(train_dataloader)\n    avg_train_r2 = sum(train_r2_scores) / len(train_r2_scores)\n\n\n    # ---------------- Validation ----------------\n    model.eval()\n    val_loss = 0\n    val_r2_scores = []\n\n    with torch.no_grad():\n        for imgs, _, y in tqdm(val_dataloader, desc=f\"[Val] Epoch {epoch}\"):\n\n            imgs, y = imgs.to(device), y.to(device)\n\n            preds, loss = model(imgs, y)\n            val_loss += loss.item()\n\n            preds5 = torch.cat([\n                preds,\n                (preds[:, 2] + preds[:, 0]).unsqueeze(1),  # GDM_g = Dry_Clover + Dry_Green\n                preds.sum(dim=1).unsqueeze(1)             # Dry_Total_g = all three\n            ], dim=1)\n\n            y5 = torch.cat([\n                y,\n                (y[:, 2] + y[:, 0]).unsqueeze(1),  # GDM_g = Dry_Clover + Dry_Green\n                y.sum(dim=1).unsqueeze(1)          # Dry_Total_g = all three\n            ], dim=1)\n\n            val_r2_scores.append(weighted_r2(y5, preds5, weights).item())\n\n    avg_val_loss = val_loss / len(val_dataloader)\n    avg_val_r2 = sum(val_r2_scores) / len(val_r2_scores)\n    val_losses.append(avg_val_loss)\n    train_losses.append(avg_train_loss)\n    val_r2_history.append(avg_val_r2)\n    train_r2_history.append(avg_train_r2)\n\n    print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | \"\n          f\"Train R2: {avg_train_r2:.4f} | Val Loss: {avg_val_loss:.4f} | Val R2: {avg_val_r2:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:18:58.539780Z","iopub.execute_input":"2025-12-01T13:18:58.540042Z","execution_failed":"2025-12-01T13:22:36.070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# ---------------------------\n# LOSS PLOTS\n# ---------------------------\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# ---------------------------\n# R² PLOTS\n# ---------------------------\nplt.figure(figsize=(10, 5))\nplt.plot(train_r2_history, label=\"Train R2\")\nplt.plot(val_r2_history, label=\"Validation R2\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"R² Score\")\nplt.title(\"Training vs Validation R²\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-01T13:22:36.070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\n\nmodel.eval()\nrows = []\n\ntarget_cols = [\n    \"Dry_Green_g\",\n    \"Dry_Dead_g\",\n    \"Dry_Clover_g\",\n    \"GDM_g\",\n    \"Dry_Total_g\",\n]\n\ntest_dataset = Image2BioMassTestDataset(\n    dataset_path=\"/kaggle/input/csiro-biomass/\",\n    # img_transform=image_transform,\n)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\nwith torch.no_grad():\n    for imgs, _, sample_ids in tqdm(test_dataloader, desc=\"Inference\"):\n        \n        imgs = imgs.to(device)\n\n        # (B, 3) - model outputs: [Dry_Green_g, Dry_Dead_g, Dry_Clover_g]\n        preds3, _ = model(imgs, y=None)\n\n        # reverse log transform -> still (B,3)\n        # print(preds3)\n        preds3 = target_untransform(preds3).cpu().numpy()\n\n        # extract 3 predictions in the correct order\n        dg = preds3[:, 0]  # Dry_Green_g\n        dd = preds3[:, 1]  # Dry_Dead_g\n        dc = preds3[:, 2]  # Dry_Clover_g\n\n        # compute extra targets\n        gdm = dg + dc\n        dry_total = dg + dd + dc\n\n        preds5 = np.stack([dg, dd, dc, gdm, dry_total], axis=1)\n        np.set_printoptions(suppress=True, precision=4)\n        # print(preds5)\n\n        # build submission rows\n        for sid, pred_vec in zip(sample_ids, preds5):\n            for col, value in zip(target_cols, pred_vec):\n                rows.append({\n                    \"sample_id\": f\"{sid}__{col}\",\n                    \"target\": float(value)\n                })\n\ndf_submit = pd.DataFrame(rows)\ndf_submit.to_csv(\"submission.csv\", index=False)\nprint(\"Saved submission.csv\")\ndf_submit.head(20)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-01T13:22:36.070Z"}},"outputs":[],"execution_count":null}]}